{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "TaskObject = \"\""
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from notebookutils import mssparkutils\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import date\n",
        "from datetime import datetime\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from delta.tables import *\n",
        "#TaskObject = \"{\\\"TaskInstanceId\\\":10,\\\"TaskMasterId\\\":2,\\\"TaskStatus\\\":\\\"InProgress\\\",\\\"TaskType\\\":\\\"Execute Synapse Notebook\\\",\\\"Enabled\\\":1,\\\"ExecutionUid\\\":\\\"29283663-9778-4c37-9100-1307f98ac2f4\\\",\\\"NumberOfRetries\\\":2,\\\"DegreeOfCopyParallelism\\\":1,\\\"KeyVaultBaseUrl\\\":\\\"https://ark-stg-kv-ads-ikjn.vault.azure.net/\\\",\\\"ScheduleMasterId\\\":\\\"-4\\\",\\\"TaskGroupConcurrency\\\":\\\"10\\\",\\\"TaskGroupPriority\\\":0,\\\"TaskExecutionType\\\":\\\"ADF\\\",\\\"ExecutionEngine\\\":{\\\"EngineId\\\":-2,\\\"EngineName\\\":\\\"arkstgsynwadsikjn\\\",\\\"SystemType\\\":\\\"Synapse\\\",\\\"ResourceGroup\\\":\\\"LockboxDev01\\\",\\\"SubscriptionId\\\":\\\"687fe1ae-a520-4f86-b921-a80664c40f9b\\\",\\\"ADFPipeline\\\":\\\"GPL_SparkNotebookExecution_Azure\\\",\\\"EngineJson\\\":\\\"{\\\\r\\\\n            \\\\\\\"endpoint\\\\\\\": \\\\\\\"https://arkstgsynwadsikjn.dev.azuresynapse.net\\\\\\\", \\\\\\\"DeltaProcessingNotebook\\\\\\\": \\\\\\\"DeltaProcessingNotebook\\\\\\\", \\\\\\\"PurviewAccountName\\\\\\\": \\\\\\\"arkstgpurads\\\\\\\", \\\\\\\"DefaultSparkPoolName\\\\\\\":\\\\\\\"Dummy\\\\\\\"\\\\r\\\\n        }\\\",\\\"TaskDatafactoryIR\\\":\\\"Azure\\\",\\\"JsonProperties\\\":{\\\"endpoint\\\":\\\"https://arkstgsynwadsikjn.dev.azuresynapse.net\\\",\\\"DeltaProcessingNotebook\\\":\\\"DeltaProcessingNotebook\\\",\\\"PurviewAccountName\\\":\\\"arkstgpurads\\\",\\\"DefaultSparkPoolName\\\":\\\"Dummy\\\"}},\\\"Source\\\":{\\\"System\\\":{\\\"SystemId\\\":-8,\\\"SystemServer\\\":\\\"https://arkstgdlsadsikjnadsl.dfs.core.windows.net\\\",\\\"AuthenticationType\\\":\\\"MSI\\\",\\\"Type\\\":\\\"ADLS\\\",\\\"Username\\\":null,\\\"Container\\\":\\\"datalakelanding\\\"},\\\"Instance\\\":{\\\"SourceRelativePath\\\":\\\"{y}/{m}/{d}/\\\",\\\"TargetRelativePath\\\":\\\"{y}/{m}/{d}/\\\"},\\\"DataFileName\\\":\\\"TestHubSpot\\\",\\\"RelativePath\\\":\\\"{y}/{m}/{d}/\\\",\\\"SchemaFileName\\\":\\\"\\\",\\\"Type\\\":\\\"Notebook-Optional\\\",\\\"WriteSchemaToPurview\\\":\\\"Disabled\\\"},\\\"Target\\\":{\\\"System\\\":{\\\"SystemId\\\":-8,\\\"SystemServer\\\":\\\"https://arkstgdlsadsikjnadsl.dfs.core.windows.net\\\",\\\"AuthenticationType\\\":\\\"MSI\\\",\\\"Type\\\":\\\"ADLS\\\",\\\"Username\\\":null,\\\"Container\\\":\\\"datalakelanding\\\"},\\\"Instance\\\":{\\\"SourceRelativePath\\\":\\\"{y}/{m}/{d}/\\\",\\\"TargetRelativePath\\\":\\\"{y}/{m}/{d}/\\\"},\\\"DataFileName\\\":\\\"HubSpotParquet\\\",\\\"RelativePath\\\":\\\"{y}/{m}/{d}/\\\",\\\"SchemaFileName\\\":\\\"\\\",\\\"Type\\\":\\\"Notebook-Optional\\\",\\\"WriteSchemaToPurview\\\":\\\"Disabled\\\"},\\\"TMOptionals\\\":{\\\"CustomDefinitions\\\":\\\"\\\",\\\"ExecuteNotebook\\\":\\\"CustomHubSpot-JsonToParquet\\\",\\\"Purview\\\":\\\"Disabled\\\",\\\"QualifiedIDAssociation\\\":\\\"TaskMasterId\\\",\\\"UseNotebookActivity\\\":\\\"Enabled\\\"}}\"\n",
        "TaskObjectJson = json.loads(TaskObject)\n",
        "now = datetime.now()\n",
        "\n",
        "\n",
        "#SOURCE\n",
        "Source = TaskObjectJson['Source']['System']['Container'] + \"@\" + TaskObjectJson['Source']['System']['SystemServer'].replace(\"https://\",\"\") + \"/\"\n",
        "Source = Source + TaskObjectJson['Source']['Instance']['SourceRelativePath'] + \"/\" + TaskObjectJson['Source']['DataFileName']\n",
        "Source = Source.replace('//', '/')\n",
        "Source = \"abfss://\" + Source\n",
        "Source = Source.replace(\"{yyyy}\", \"%Y\")\n",
        "Source = Source.replace(\"{MM}\", \"%m\")\n",
        "Source = Source.replace(\"{dd}\", \"%d\")\n",
        "Source = Source.replace(\"{hh}\", \"%H\")\n",
        "Source = Source.replace(\"{mm}\", \"%M\")\n",
        "Source = now.strftime(Source)\n",
        "print(\"SOURCE: \" + Source)\n",
        "\n",
        "\n",
        "\n",
        "#TARGET\n",
        "Target = TaskObjectJson['Target']['System']['Container'] + \"@\" + TaskObjectJson['Target']['System']['SystemServer'].replace(\"https://\",\"\") + \"/\"\n",
        "Target = Target + TaskObjectJson['Target']['Instance']['TargetRelativePath'] + \"/\" + TaskObjectJson['Target']['DataFileName']\n",
        "Target = Target.replace(\"{yyyy}\", \"%Y\")\n",
        "Target = Target.replace(\"{MM}\", \"%m\")\n",
        "Target = Target.replace(\"{dd}\", \"%d\")\n",
        "Target = Target.replace(\"{hh}\", \"%H\")\n",
        "Target = Target.replace(\"{mm}\", \"%M\")\n",
        "Target = now.strftime(Target)\n",
        "\n",
        "TargetDelta = Target +\"/DeltaTable\"\n",
        "TargetDelta = TargetDelta.replace('//', '/')\n",
        "TargetDelta = \"abfss://\" + TargetDelta\n",
        "Target = Target + \"/LatestVersion\"\n",
        "Target = Target.replace('//', '/')\n",
        "Target = \"abfss://\" + Target\n",
        "print(\"SAVE TARGET: \" + Target)\n",
        "print(\"DELTA SAVE TARGET: \" + TargetDelta ) \n",
        "\n",
        "sourceDF = spark.read.option(\"multiline\",\"true\").json(Source)\n",
        "\n",
        "pdf = sourceDF.toPandas()\n",
        "t = pdf.iat[0,0]\n",
        "fullList = []\n",
        "\n",
        "for x in t:\n",
        "    d = x.asDict() \n",
        "    fullList.append(d)\n",
        "\n",
        "pdf = pd.DataFrame.from_records(fullList)\n",
        "df=spark.createDataFrame(pdf) \n",
        "df = df.select(\"id\",\"firstname\",\"lastname\",\"email\",\"phone\",\"company\",\"createdAt\",\"updatedAt\")\n",
        "#df.show()\n",
        "#Convert processed dataflow to delta\n",
        "sql = 'describe detail \"' + TargetDelta + '\"'\n",
        "mergeCondition = \"oldData.id\" + \" = newData.id\"\n",
        "\n",
        "try:\n",
        "    if (spark.sql(sql).collect()[0].asDict()['format'] == 'delta'):\n",
        "        print(\"Table already exists. Performing Merge\")\n",
        "        #olddt = spark.read.format('delta').load(TargetDelta)\n",
        "        olddt = DeltaTable.forPath(spark, TargetDelta) \n",
        "        olddt.alias(\"oldData\").merge(df.alias(\"newData\"),mergeCondition).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
        "\n",
        "        dfd = spark.read.format(\"delta\").load(TargetDelta)\n",
        "        dfd.write.format(\"parquet\").mode(\"overwrite\").save(Target) \n",
        "\n",
        "    else:\n",
        "        print(\"Table does not exist. No error, creating new Delta Table.\")    \n",
        "        df.write.format(\"delta\").save(TargetDelta)\n",
        "        df.write.format(\"parquet\").mode(\"overwrite\").save(Target) \n",
        "\n",
        "except:\n",
        "    print(\"Table does not exist. Creating new Delta Table.\")    \n",
        "    df.write.format(\"delta\").save(TargetDelta)\n",
        "    df.write.format(\"parquet\").mode(\"overwrite\").save(Target) \n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 97,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}