{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\r\n",
        "from notebookutils import mssparkutils\r\n",
        "import pandas as pd\r\n",
        "from pyspark.sql import Row\r\n",
        "Source = 'abfss://datalakelanding@arkstgdlsadsaqyeadsl.dfs.core.windows.net/accounts-response.json'\r\n",
        "sourceDF = spark.read.option(\"multiline\",\"true\").json(Source)\r\n",
        "sourceDF.show()\r\n",
        "#used for outputting to json in next cell block\r\n",
        "Target = 'abfss://datalakelanding@arkstgdlsadsaqyeadsl.dfs.core.windows.net/test-response.json'\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#flattening of json stored in a dataframe (elements column, first row)\r\n",
        "#using function\r\n",
        "\r\n",
        "#This function will take a dictionary and build it out completely for columns in a dataframe (so a nested dictionary / json )\r\n",
        "def as_row(obj):\r\n",
        "    if isinstance(obj, dict):\r\n",
        "        dictionary = {k: as_row(v) for k, v in obj.items()}\r\n",
        "        return Row(**dictionary)\r\n",
        "    elif isinstance(obj, list):\r\n",
        "        return [as_row(v) for v in obj]\r\n",
        "    else:\r\n",
        "        return obj\r\n",
        "\r\n",
        "#collect each array object of 'elements' and convert it to a list of dictionaries\r\n",
        "ss=sourceDF.select(\"elements\").collect()\r\n",
        "output=[i[0] for i in ss]\r\n",
        "fullList = []\r\n",
        "for obj in output:\r\n",
        "    for row in obj:\r\n",
        "        fullList.append(row.asDict(True))\r\n",
        "\r\n",
        "#we now want to iterate this list of dictionaries (or list of jsons) for manipulation and re-building into an amended list\r\n",
        "amendedList = []\r\n",
        "for obj in fullList:\r\n",
        "    obj[\"id\"] = str(obj[\"id\"])\r\n",
        "    row = as_row(obj)\r\n",
        "    amendedList.append(row)\r\n",
        "\r\n",
        "\r\n",
        "#for testing\r\n",
        "#for x in amendedList:\r\n",
        "#    print(type(x))\r\n",
        "outDF = spark.createDataFrame(amendedList)\r\n",
        "outDF.show()\r\n",
        "\r\n",
        "#this was for testing writing to json \r\n",
        "#newDF.write.format(\"json\").mode(\"overwrite\").save(Target)\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#taking a json array in dataframe (elements column, first row) and making it multiple rows of json objects in a new dataframe\r\n",
        "\r\n",
        "#collect each array object of 'elements' and convert it to a list of dictionaries\r\n",
        "ss=sourceDF.select(\"elements\").collect()\r\n",
        "output=[i[0] for i in ss]\r\n",
        "fullList = []\r\n",
        "for obj in output:\r\n",
        "    for row in obj:\r\n",
        "        fullList.append(row.asDict(True))\r\n",
        "\r\n",
        "#we now want to iterate this list of dictionaries (or list of jsons) for manipulation and re-building into an amended list\r\n",
        "amendedList = []\r\n",
        "for obj in fullList:\r\n",
        "    obj[\"id\"] = str(obj[\"id\"])\r\n",
        "    amendedList.append(obj)\r\n",
        "\r\n",
        "\r\n",
        "#re-build - note: pyspark is a bit funky currently with the first run using pandas, will throw a warning and hide output, but normally still executes full code block (check for successful run)\r\n",
        "pdDF = pd.DataFrame([str(e) for e in amendedList])\r\n",
        "columns = [\"elements\"]\r\n",
        "outDF = spark.createDataFrame(pdDF, columns)\r\n",
        "outDF.show()\r\n",
        "\r\n",
        "#this was for testing writing to json \r\n",
        "#newDF.write.format(\"json\").mode(\"overwrite\").save(Target)\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#output = pd.DataFrame()\r\n",
        "#for x in amendedList:\r\n",
        "#    output = output.append(x, ignore_index=True)\r\n",
        "#sparkDF=spark.createDataFrame(output)\r\n",
        "#sparkDF.show() \r\n",
        "#for a in amendedList:\r\n",
        "  #  print(e)\r\n",
        "\r\n",
        "\r\n",
        "#df = pd.DataFrame(amendedList, columns=['elements'])\r\n",
        "#df = spark.createDataFrame(df)\r\n",
        "#df.show()\r\n"
      ],
      "outputs": [],
      "execution_count": 99,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\r\n",
        "{\r\n",
        "    \"sink\": {\r\n",
        "        \"storeSettings\": {\r\n",
        "            \"type\": \"AzureBlobFSWriteSettings\"\r\n",
        "        },\r\n",
        "        \"type\": \"ParquetSink\"\r\n",
        "    },\r\n",
        "    \"mappings\": [\r\n",
        "        {\r\n",
        "            \"source\": {\r\n",
        "                \"name\": \"LAST_NAME\",\r\n",
        "                \"type\": \"String\",\r\n",
        "                \"physicalType\": \"VARCHAR2\"\r\n",
        "            },\r\n",
        "            \"sink\": {\r\n",
        "                \"name\": \"LASTNAME\",\r\n",
        "                \"type\": \"UTF8\",\r\n",
        "                \"physicalType\": \"VARCHAR2\"\r\n",
        "            }\r\n",
        "        },\r\n",
        "        {\r\n",
        "            \"source\": {\r\n",
        "                \"name\": \"UNIQUE_EMAIL\",\r\n",
        "                \"type\": \"String\",\r\n",
        "                \"physicalType\": \"VARCHAR2\"\r\n",
        "            },\r\n",
        "            \"sink\": {\r\n",
        "                \"name\": \"UNIQUEEMAIL\",\r\n",
        "                \"type\": \"UTF8\",\r\n",
        "                \"physicalType\": \"VARCHAR2\"\r\n",
        "            }\r\n",
        "        },\r\n",
        "        {\r\n",
        "            \"source\": {\r\n",
        "                \"name\": \"FIRST_NAME\",\r\n",
        "                \"type\": \"String\",\r\n",
        "                \"physicalType\": \"VARCHAR2\"\r\n",
        "            },\r\n",
        "            \"sink\": {\r\n",
        "                \"name\": \"FIRSTNAME\",\r\n",
        "                \"type\": \"UTF8\",\r\n",
        "                \"physicalType\": \"VARCHAR2\"\r\n",
        "            }\r\n",
        "        },\r\n",
        "        {\r\n",
        "            \"source\": {\r\n",
        "                \"name\": \"PERSON_ID\",\r\n",
        "                \"type\": \"Int64\",\r\n",
        "                \"physicalType\": \"NUMBER\"\r\n",
        "            },\r\n",
        "            \"sink\": {\r\n",
        "                \"name\": \"PERSONID\",\r\n",
        "                \"type\": \"Int64\",\r\n",
        "                \"physicalType\": \"NUMBER\"\r\n",
        "            }\r\n",
        "        }\r\n",
        "    ]\r\n",
        "    \r\n",
        "}\r\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}