{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "TaskObject = \" \\\r\n",
        "{\\\r\n",
        "    \\\"TaskInstanceId\\\": 28,\\\r\n",
        "    \\\"TaskMasterId\\\": 2,\\\r\n",
        "    \\\"TaskStatus\\\": \\\"InProgress\\\",\\\r\n",
        "    \\\"TaskType\\\": \\\"Execute Synapse Notebook\\\",\\\r\n",
        "    \\\"Enabled\\\": 1,\\\r\n",
        "    \\\"ExecutionUid\\\": \\\"f3156876-086d-4915-b1b1-0b445810142c\\\",\\\r\n",
        "    \\\"NumberOfRetries\\\": 0,\\\r\n",
        "    \\\"DegreeOfCopyParallelism\\\": 1,\\\r\n",
        "    \\\"KeyVaultBaseUrl\\\": \\\"https://ads-dev-kv-ads-we7y.vault.azure.net/\\\",\\\r\n",
        "    \\\"ScheduleMasterId\\\": \\\"-4\\\",\\\r\n",
        "    \\\"TaskGroupConcurrency\\\": \\\"10\\\",\\\r\n",
        "    \\\"TaskGroupPriority\\\": 0,\\\r\n",
        "    \\\"TaskExecutionType\\\": \\\"ADF\\\",\\\r\n",
        "    \\\"ExecutionEngine\\\": {\\\r\n",
        "        \\\"EngineId\\\": -2,\\\r\n",
        "        \\\"EngineName\\\": \\\"adsdevsynwadswe7y\\\",\\\r\n",
        "        \\\"SystemType\\\": \\\"Synapse\\\",\\\r\n",
        "        \\\"ResourceGroup\\\": \\\"gfd1\\\",\\\r\n",
        "        \\\"SubscriptionId\\\": \\\"035a1364-f00d-48e2-b582-4fe125905ee3\\\",\\\r\n",
        "        \\\"ADFPipeline\\\": \\\"GPL_SparkNotebookExecution_Azure\\\",\\\r\n",
        "        \\\"TaskDatafactoryIR\\\": \\\"Azure\\\",\\\r\n",
        "        \\\"JsonProperties\\\": {\\\r\n",
        "            \\\"endpoint\\\": \\\"https://adsdevsynwadswe7y.dev.azuresynapse.net\\\",\\\r\n",
        "            \\\"DeltaProcessingNotebook\\\": \\\"DeltaProcessingNotebook\\\",\\\r\n",
        "            \\\"PurviewAccountName\\\": \\\"adsdevpuradswe7y\\\",\\\r\n",
        "            \\\"DefaultSparkPoolName\\\": \\\"adsdevsynspads\\\"\\\r\n",
        "        }\\\r\n",
        "    },\\\r\n",
        "    \\\"Source\\\": {\\\r\n",
        "        \\\"System\\\": {\\\r\n",
        "            \\\"SystemId\\\": -4,\\\r\n",
        "            \\\"SystemServer\\\": \\\"https://adsdevdlsadswe7yadsl.dfs.core.windows.net\\\",\\\r\n",
        "            \\\"AuthenticationType\\\": \\\"MSI\\\",\\\r\n",
        "            \\\"Type\\\": \\\"ADLS\\\",\\\r\n",
        "            \\\"Username\\\": null,\\\r\n",
        "            \\\"Container\\\": \\\"datalakeraw\\\"\\\r\n",
        "        },\\\r\n",
        "        \\\"Instance\\\": {\\\r\n",
        "            \\\"SourceRelativePath\\\": \\\"/samples/sif/\\\" \\\r\n",
        "        },\\\r\n",
        "        \\\"DataFileName\\\": \\\"CalendarDate.json\\\",\\\r\n",
        "        \\\"RelativePath\\\": \\\"/samples/sif/\\\",\\\r\n",
        "        \\\"SchemaFileName\\\": \\\"\\\",\\\r\n",
        "        \\\"Type\\\": \\\"Notebook-Optional\\\",\\\r\n",
        "        \\\"WriteSchemaToPurview\\\": \\\"Disabled\\\"\\\r\n",
        "    },\\\r\n",
        "    \\\"Target\\\": {\\\r\n",
        "        \\\"System\\\": {\\\r\n",
        "            \\\"SystemId\\\": -4,\\\r\n",
        "            \\\"SystemServer\\\": \\\"https://adsdevdlsadswe7yadsl.dfs.core.windows.net\\\",\\\r\n",
        "            \\\"AuthenticationType\\\": \\\"MSI\\\",\\\r\n",
        "            \\\"Type\\\": \\\"ADLS\\\",\\\r\n",
        "            \\\"Username\\\": null,\\\r\n",
        "            \\\"Container\\\": \\\"datalakeraw\\\"\\\r\n",
        "        },\\\r\n",
        "        \\\"Instance\\\": {\\\r\n",
        "            \\\"SourceRelativePath\\\": \\\"/samples/sif/\\\", \\\r\n",
        "            \\\"TargetRelativePath\\\": \\\"/samples/sif/delta/calendardate/\\\"\\\r\n",
        "        },\\\r\n",
        "        \\\"DataFileName\\\": \\\"CalendarDate.parquet\\\",\\\r\n",
        "        \\\"RelativePath\\\": \\\"/samples/sif/delta/calendardate/\\\",\\\r\n",
        "        \\\"SchemaFileName\\\": \\\"CalendarDate.json\\\",\\\r\n",
        "        \\\"Type\\\": \\\"Notebook-Optional\\\",\\\r\n",
        "        \\\"WriteSchemaToPurview\\\": \\\"Disabled\\\"\\\r\n",
        "    },\\\r\n",
        "    \\\"TMOptionals\\\": {\\\r\n",
        "        \\\"CustomDefinitions\\\": \\\"SparkDatabaseName=sif,EntityPrimaryKey=CalendarDateRefId\\\",\\\r\n",
        "        \\\"ExecuteNotebook\\\": \\\"SIFParameterizedJson\\\",\\\r\n",
        "        \\\"Purview\\\": \\\"Disabled\\\",\\\r\n",
        "        \\\"QualifiedIDAssociation\\\": \\\"TaskMasterId\\\",\\\r\n",
        "        \\\"UseNotebookActivity\\\": \\\"Enabled\\\"\\\r\n",
        "    }\\\r\n",
        "}\" \r\n",
        "\r\n",
        "print(TaskObject)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "adsdevsynspads",
              "session_id": 67,
              "statement_id": 1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-07-10T09:43:44.9938077Z",
              "session_start_time": "2022-07-10T09:43:45.0393695Z",
              "execution_start_time": "2022-07-10T09:44:54.2608761Z",
              "execution_finish_time": "2022-07-10T09:44:54.4114159Z"
            },
            "text/plain": "StatementMeta(adsdevsynspads, 67, 1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " {    \"TaskInstanceId\": 28,    \"TaskMasterId\": 2,    \"TaskStatus\": \"InProgress\",    \"TaskType\": \"Execute Synapse Notebook\",    \"Enabled\": 1,    \"ExecutionUid\": \"f3156876-086d-4915-b1b1-0b445810142c\",    \"NumberOfRetries\": 0,    \"DegreeOfCopyParallelism\": 1,    \"KeyVaultBaseUrl\": \"https://ads-dev-kv-ads-we7y.vault.azure.net/\",    \"ScheduleMasterId\": \"-4\",    \"TaskGroupConcurrency\": \"10\",    \"TaskGroupPriority\": 0,    \"TaskExecutionType\": \"ADF\",    \"ExecutionEngine\": {        \"EngineId\": -2,        \"EngineName\": \"adsdevsynwadswe7y\",        \"SystemType\": \"Synapse\",        \"ResourceGroup\": \"gfd1\",        \"SubscriptionId\": \"035a1364-f00d-48e2-b582-4fe125905ee3\",        \"ADFPipeline\": \"GPL_SparkNotebookExecution_Azure\",        \"TaskDatafactoryIR\": \"Azure\",        \"JsonProperties\": {            \"endpoint\": \"https://adsdevsynwadswe7y.dev.azuresynapse.net\",            \"DeltaProcessingNotebook\": \"DeltaProcessingNotebook\",            \"PurviewAccountName\": \"adsdevpuradswe7y\",            \"DefaultSparkPoolName\": \"adsdevsynspads\"        }    },    \"Source\": {        \"System\": {            \"SystemId\": -4,            \"SystemServer\": \"https://adsdevdlsadswe7yadsl.dfs.core.windows.net\",            \"AuthenticationType\": \"MSI\",            \"Type\": \"ADLS\",            \"Username\": null,            \"Container\": \"datalakeraw\"        },        \"Instance\": {            \"SourceRelativePath\": \"/samples/sif/\"         },        \"DataFileName\": \"CalendarDate.json\",        \"RelativePath\": \"/samples/sif/\",        \"SchemaFileName\": \"\",        \"Type\": \"Notebook-Optional\",        \"WriteSchemaToPurview\": \"Disabled\"    },    \"Target\": {        \"System\": {            \"SystemId\": -4,            \"SystemServer\": \"https://adsdevdlsadswe7yadsl.dfs.core.windows.net\",            \"AuthenticationType\": \"MSI\",            \"Type\": \"ADLS\",            \"Username\": null,            \"Container\": \"datalakeraw\"        },        \"Instance\": {            \"SourceRelativePath\": \"/samples/sif/\",             \"TargetRelativePath\": \"/samples/sif/delta/calendardate/\"        },        \"DataFileName\": \"CalendarDate.parquet\",        \"RelativePath\": \"/samples/sif/delta/calendardate/\",        \"SchemaFileName\": \"CalendarDate.json\",        \"Type\": \"Notebook-Optional\",        \"WriteSchemaToPurview\": \"Disabled\"    },    \"TMOptionals\": {        \"CustomDefinitions\": \"SparkDatabaseName=sif,EntityPrimaryKey=CalendarDateRefId\",        \"ExecuteNotebook\": \"SIFParameterizedJson\",        \"Purview\": \"Disabled\",        \"QualifiedIDAssociation\": \"TaskMasterId\",        \"UseNotebookActivity\": \"Enabled\"    }}\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extract Variables from TaskObject**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\r\n",
        "import json\r\n",
        "from pyspark.sql import Row\r\n",
        "from pyspark.sql.types import *\r\n",
        "from pyspark.sql.functions import *\r\n",
        "from pyspark.sql.functions import trim\r\n",
        "\r\n",
        "session_id = random.randint(0,1000000)\r\n",
        "#invalid source\r\n",
        "##TaskObject = \"{\\\"TaskInstanceId\\\":1,\\\"TaskMasterId\\\":2,\\\"TaskStatus\\\":\\\"InProgress\\\",\\\"TaskType\\\":\\\"TestTask Type Name\\\",\\\"Enabled\\\":1,\\\"ExecutionUid\\\":\\\"8448eabb-9ba4-4779-865b-29e973431273\\\",\\\"NumberOfRetries\\\":0,\\\"DegreeOfCopyParallelism\\\":1,\\\"KeyVaultBaseUrl\\\":\\\"https://ark-stg-kv-ads-irud.vault.azure.net/\\\",\\\"ScheduleMasterId\\\":\\\"-4\\\",\\\"TaskGroupConcurrency\\\":\\\"10\\\",\\\"TaskGroupPriority\\\":0,\\\"TaskExecutionType\\\":\\\"ADF\\\",\\\"ExecutionEngine\\\":{\\\"EngineId\\\":-1,\\\"EngineName\\\":\\\"ark-stg-adf-ads-irud\\\",\\\"SystemType\\\":\\\"Datafactory\\\",\\\"ResourceGroup\\\":\\\"dlzdev04\\\",\\\"SubscriptionId\\\":\\\"ed1206e0-17c7-4bc2-ad4b-f8d4dab9284f\\\",\\\"ADFPipeline\\\":\\\"GPL_AzureSqlTable_NA_AzureBlobFS_Parquet_Azure\\\",\\\"EngineJson\\\":\\\"{}\\\",\\\"TaskDatafactoryIR\\\":\\\"Azure\\\",\\\"JsonProperties\\\":{}},\\\"Source\\\":{\\\"System\\\":{\\\"SystemId\\\":-8,\\\"SystemServer\\\":\\\"https://arkstgdlsadsirudadsl.dfs.core.windows.net\\\",\\\"AuthenticationType\\\":\\\"MSI\\\",\\\"Type\\\":\\\"ADLS\\\",\\\"Username\\\":null,\\\"Container\\\":\\\"datalakelanding\\\"},\\\"Instance\\\":{\\\"TargetRelativePath\\\":\\\"\\\"},\\\"DataFileName\\\":\\\"TestFile.parquet\\\",\\\"RelativePath\\\":\\\"\\\",\\\"SchemaFileName\\\":\\\"TestFile.json\\\"},\\\"Target\\\":{\\\"System\\\":{\\\"SystemId\\\":-8,\\\"SystemServer\\\":\\\"https://arkstgdlsadsirudadsl.dfs.core.windows.net\\\",\\\"AuthenticationType\\\":\\\"MSI\\\",\\\"Type\\\":\\\"ADLS\\\",\\\"Username\\\":null,\\\"Container\\\":\\\"datalakelanding\\\"},\\\"Instance\\\":{\\\"TargetRelativePath\\\":\\\"\\\"},\\\"DataFileName\\\":\\\"TestFile.parquet\\\",\\\"RelativePath\\\":\\\"\\\",\\\"SchemaFileName\\\":\\\"TestFile.json\\\",\\\"Type\\\":\\\"Parquet\\\"}}\"\r\n",
        "#valid source\r\n",
        "#TaskObject = \"{\\\"TaskInstanceId\\\":1,\\\"TaskMasterId\\\":2,\\\"TaskStatus\\\":\\\"InProgress\\\",\\\"TaskType\\\":\\\"TestTask Type Name\\\", \\\"Enabled\\\":1,\\\"ExecutionUid\\\":\\\"8448eabb-9ba4-4779-865b-29e973431273\\\",\\\"NumberOfRetries\\\":0,\\\"DegreeOfCopyParallelism\\\":1, \\\"KeyVaultBaseUrl\\\":\\\"https://ads-dev-kv-ads-ic038069.vault.azure.net/\\\",\\\"ScheduleMasterId\\\":\\\"-4\\\",\\\"TaskGroupConcurrency\\\":\\\"10\\\", \\\"TaskGroupPriority\\\":0,\\\"TaskExecutionType\\\":\\\"ADF\\\",\\\"ExecutionEngine\\\":{\\\"EngineId\\\":-1,\\\"EngineName\\\":\\\"ads-dev-kv-ads-ic038069\\\", \\\"SystemType\\\":\\\"Microsoft.Synapse/workspaces\\\",\\\"ResourceGroup\\\":\\\"sifgofast\\\",\\\"SubscriptionId\\\":\\\"cd486ba9-eef3-466d-b16c-7f1b2941ae9d\\\", \\\"ADFPipeline\\\":\\\"GPL_AzureSqlTable_NA_AzureBlobFS_Parquet_Azure\\\",\\\"EngineJson\\\":\\\"{}\\\",\\\"TaskDatafactoryIR\\\":\\\"Azure\\\", \\\"JsonProperties\\\":{}},\\\"Source\\\":{\\\"System\\\":{\\\"SystemId\\\":-8,\\\"SystemServer\\\":\\\"https://adsdevdlsadsic03adsl.blob.core.windows.net\\\", \\\"AuthenticationType\\\":\\\"MSI\\\",\\\"Type\\\":\\\"ADLS\\\",\\\"Username\\\":null,\\\"Container\\\":\\\"adsdevdlsadsic03\\\"},\\\"Instance\\\":\\\"\\\",{\\\"TargetRelativePath\\\":\\\"synapse/sif\\\"}, \\\"DataFileName\\\":\\\"StudentPersonal.parquet\\\",\\\"SourceRelativePath\\\":\\\"synapse/sif\\\",\\\"SchemaFileName\\\":\\\"StudentPersonal.json\\\",\\\"Type\\\":\\\"Parquet\\\"}, \\\"Target\\\":{\\\"System\\\":{\\\"SystemId\\\":-8,\\\"SystemServer\\\":\\\"https://adsdevdlsadsic03adsl.blob.core.windows.net\\\", \\\"AuthenticationType\\\":\\\"MSI\\\",\\\"Type\\\":\\\"ABS\\\",\\\"Username\\\":null,\\\"Container\\\":\\\"adsdevdlsadsic03\\\"}, \\\"Instance\\\":{\\\"TargetRelativePath\\\":\\\"\\\"},\\\"DataFileName\\\":\\\"StudentPersonal.parquet\\\",\\\"SourceRelativePath\\\":\\\"synapse\\/sif\\\", \\\"SchemaFileName\\\":\\\"StudentPersonal.json\\\",\\\"Type\\\":\\\"Parquet\\\"}}\"\r\n",
        "TaskDict = {}\r\n",
        "OutputDict = {}\r\n",
        "TaskObjectJson = json.loads(TaskObject)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "##we want to delete EngineJson as it causes issues when converting back to a json and it is not needed as its properties are within JsonProperties as children\r\n",
        "try:\r\n",
        "    del TaskObjectJson['ExecutionEngine']['EngineJson']\r\n",
        "except:\r\n",
        "    print(\"No EngineJson Found\")\r\n",
        "\r\n",
        "Source = TaskObjectJson['Source']['System']['Container'] + \"@\" + TaskObjectJson['Source']['System']['SystemServer'].replace(\"https://\",\"\").replace(\"blob\",\"dfs\") + \"/\"\r\n",
        "Source = Source.replace('///', '/')\r\n",
        "Source = Source.replace('//', '/')\r\n",
        "print(Source)\r\n",
        "Source = Source + TaskObjectJson['Source'][\"Instance\"]['SourceRelativePath']\r\n",
        "print(Source)\r\n",
        "Source = Source  +\"/\" + TaskObjectJson['Source']['DataFileName'] \r\n",
        "print(Source)\r\n",
        "Source = Source.replace('///', '/')\r\n",
        "Source = Source.replace('//', '/')\r\n",
        "print(Source)\r\n",
        "Source = \"abfss://\" + Source \r\n",
        "print(Source)\r\n",
        "\r\n",
        "Target = TaskObjectJson['Target']['System']['Container'] + \"@\" + TaskObjectJson['Target']['System']['SystemServer'].replace(\"https://\",\"\").replace(\"blob\",\"dfs\") + \"/\"\r\n",
        "Target = Target.replace('///', '/')\r\n",
        "Target = Target.replace('//', '/')\r\n",
        "print(Target)\r\n",
        "Target = Target + TaskObjectJson['Target'][\"Instance\"]['TargetRelativePath']\r\n",
        "Target = Target  +\"/\" + TaskObjectJson['Target']['DataFileName'] \r\n",
        "print(Target)\r\n",
        "Target = Target.replace('///', '/')\r\n",
        "Target = Target.replace('//', '/')\r\n",
        "print(Target)\r\n",
        "Target = \"abfss://\" + Target \r\n",
        "print(Target)\r\n",
        "\r\n",
        "SifDbName = \"sif\"\r\n",
        "EntityPrimaryKey = \"RefId\"\r\n",
        "tmopts = TaskObjectJson['TMOptionals']['CustomDefinitions'].split(\",\")\r\n",
        "for o in tmopts:    \r\n",
        "    opt = o.split(\"=\")\r\n",
        "    print(opt)\r\n",
        "    optName = opt[0].strip()\r\n",
        "    if (optName == \"SparkDatabaseName\"):\r\n",
        "        print(\"Setting Spark Database Name\")\r\n",
        "        SifDbName = opt[1].lower().strip()        \r\n",
        "    if (optName == \"EntityPrimaryKey\"):\r\n",
        "        print(\"Setting EntityPrimaryKey\")\r\n",
        "        EntityPrimaryKey = opt[1].strip()       \r\n",
        "\r\n",
        "print(\"SifDbName:\" + SifDbName)\r\n",
        "print(\"EntityPrimaryKey:\" + EntityPrimaryKey)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "adsdevsynspads",
              "session_id": 67,
              "statement_id": 2,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-07-10T09:44:57.8763857Z",
              "session_start_time": null,
              "execution_start_time": "2022-07-10T09:44:58.0015719Z",
              "execution_finish_time": "2022-07-10T09:44:58.2038734Z"
            },
            "text/plain": "StatementMeta(adsdevsynspads, 67, 2, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No EngineJson Found\ndatalakeraw@adsdevdlsadswe7yadsl.dfs.core.windows.net/\ndatalakeraw@adsdevdlsadswe7yadsl.dfs.core.windows.net//samples/sif/\ndatalakeraw@adsdevdlsadswe7yadsl.dfs.core.windows.net//samples/sif//CalendarDate.json\ndatalakeraw@adsdevdlsadswe7yadsl.dfs.core.windows.net/samples/sif/CalendarDate.json\nabfss://datalakeraw@adsdevdlsadswe7yadsl.dfs.core.windows.net/samples/sif/CalendarDate.json\ndatalakeraw@adsdevdlsadswe7yadsl.dfs.core.windows.net/\ndatalakeraw@adsdevdlsadswe7yadsl.dfs.core.windows.net//samples/sif/delta/calendardate//CalendarDate.parquet\ndatalakeraw@adsdevdlsadswe7yadsl.dfs.core.windows.net/samples/sif/delta/calendardate/CalendarDate.parquet\nabfss://datalakeraw@adsdevdlsadswe7yadsl.dfs.core.windows.net/samples/sif/delta/calendardate/CalendarDate.parquet\n['SparkDatabaseName', 'sif']\nSetting Spark Database Name\n['EntityPrimaryKey', 'CalendarDateRefId']\nSetting EntityPrimaryKey\nSifDbName:sif\nEntityPrimaryKey:CalendarDateRefId\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load to Delta and Register External Table**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, explode_outer, from_json, lit, concat\r\n",
        "from pyspark.sql.types import StructType, ArrayType\r\n",
        "print(Source)\r\n",
        "input_df = spark.read.option(\"multiline\",\"true\").json(Source)\r\n",
        "#display(input_df)\r\n",
        "\r\n",
        "\r\n",
        "#output_df = execute_autoflatten_with_PK(input_df,None)\r\n",
        "#display(output_df)\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "adsdevsynspads",
              "session_id": 67,
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-07-10T09:45:13.1474868Z",
              "session_start_time": null,
              "execution_start_time": "2022-07-10T09:45:13.3028302Z",
              "execution_finish_time": "2022-07-10T09:45:24.0589889Z"
            },
            "text/plain": "StatementMeta(adsdevsynspads, 67, 3, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abfss://datalakeraw@adsdevdlsadswe7yadsl.dfs.core.windows.net/samples/sif/CalendarDate.json\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load to Delta**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from delta.tables import *\r\n",
        "\r\n",
        "df = input_df\r\n",
        "\r\n",
        "#SIF model uses RefId as Unique identifier in all the structures\r\n",
        "mergeCondition = \"oldData.{RefId} = newData.{RefId}\"\r\n",
        "mergeCondition = mergeCondition.replace(\"{RefId}\",EntityPrimaryKey)\r\n",
        "print(\"Merge Condition:\" + mergeCondition)\r\n",
        "\r\n",
        "\r\n",
        "print(\"SourceDT = \" + TaskObjectJson['Source']['Type']  + \", TargetDT = \" + TaskObjectJson['Target']['Type'] )\r\n",
        "#df = spark.read.load(Source, format=SourceDT)\r\n",
        "\r\n",
        "var_check = DeltaTable.isDeltaTable(spark, Target )\r\n",
        "try:\r\n",
        "    if (var_check):\r\n",
        "        print(\"Performing Merge... on Existing table\")\r\n",
        "\r\n",
        "        olddt = DeltaTable.forPath(spark, Target) \r\n",
        "\r\n",
        "        olddt.alias(\"oldData\").merge(\r\n",
        "            df.alias(\"newData\"),\r\n",
        "            mergeCondition) \\\r\n",
        "        .whenMatchedUpdateAll() \\\r\n",
        "        .whenNotMatchedInsertAll() \\\r\n",
        "        .execute()\r\n",
        "    else:\r\n",
        "        print(\"Creating new Delta Table.\")    \r\n",
        "        df.write.format(\"Delta\").save(Target)\r\n",
        "except:\r\n",
        "    print(\"Table does not exist. Creating new Delta Table.\")    \r\n",
        "    df.write.format(\"Delta\").save(Target)\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "adsdevsynspads",
              "session_id": 67,
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-07-10T09:45:28.2647227Z",
              "session_start_time": null,
              "execution_start_time": "2022-07-10T09:45:28.3721218Z",
              "execution_finish_time": "2022-07-10T09:46:16.2491451Z"
            },
            "text/plain": "StatementMeta(adsdevsynspads, 67, 4, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge Condition:oldData.CalendarDateRefId = newData.CalendarDateRefId\nSourceDT = Notebook-Optional, TargetDT = Notebook-Optional\nPerforming Merge... on Existing table\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Register as External Table**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Creating Spark Table\")\r\n",
        "df = spark.read.load(Target, format='delta')\r\n",
        "targetDB = \"sif\"\r\n",
        "targetTable = \"raw_\" + TaskObjectJson['Target']['DataFileName'].split(\".\")[0] \r\n",
        "#if the target datatype is parquet then we do not need to create a copy of the data - we can use the recently saved sink target\r\n",
        "if (TaskObjectJson['Target']['Type'] == 'Parquet'):\r\n",
        "    SnapshotTarget = Target\r\n",
        "else:\r\n",
        "    SnapshotTarget = Target + '/Snapshot/' + targetTable\r\n",
        "    #we need to update the parquet file - this is not very efficient but there isnt a current better way as delta tables are not supported for persistent tables\r\n",
        "    df.write.format(\"parquet\").mode(\"overwrite\").save(SnapshotTarget)\r\n",
        "\r\n",
        "\r\n",
        "#we need to make the DB and table lowercase as synapse persistent tables dont identify them as different identities\r\n",
        "targetDB = targetDB.lower()\r\n",
        "targetTable = targetTable.lower()\r\n",
        "\r\n",
        "#check if the specified DB / table exists - if so only do required actions.\r\n",
        "dbList = spark.catalog.listDatabases()\r\n",
        "dbExists = False\r\n",
        "for db in dbList:\r\n",
        "    if (db.name == targetDB):\r\n",
        "        dbExists = True\r\n",
        "        break\r\n",
        "if (dbExists):\r\n",
        "    print(\"DB Exists\")\r\n",
        "    tableExists = False\r\n",
        "    spark.catalog.setCurrentDatabase(targetDB)\r\n",
        "    tableList = spark.catalog.listTables()\r\n",
        "    for table in tableList:\r\n",
        "        if (table.name == targetTable):\r\n",
        "            tableExists = True\r\n",
        "            break\r\n",
        "    if (tableExists):\r\n",
        "        print(\"Table exists - nothing needed to be done\")\r\n",
        "        spark.catalog.refreshTable(targetTable)\r\n",
        "    else:\r\n",
        "        print(\"Table doesnt exist - creating\")\r\n",
        "        spark.catalog.createExternalTable(targetTable, path=SnapshotTarget, source='parquet')\r\n",
        "else:\r\n",
        "    print(\"DB Doesnt exist - creating DB and table\")\r\n",
        "    createDBString = \"CREATE DATABASE \" + targetDB \r\n",
        "    spark.sql(createDBString)\r\n",
        "    spark.catalog.setCurrentDatabase(targetDB)\r\n",
        "    spark.catalog.createExternalTable(targetTable, path=SnapshotTarget, source='parquet')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "adsdevsynspads",
              "session_id": 67,
              "statement_id": 5,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-07-10T09:46:29.9729758Z",
              "session_start_time": null,
              "execution_start_time": "2022-07-10T09:46:30.104767Z",
              "execution_finish_time": "2022-07-10T09:46:59.1399553Z"
            },
            "text/plain": "StatementMeta(adsdevsynspads, 67, 5, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Spark Table\nDB Exists\nTable exists - nothing needed to be done\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}